{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYw4YK3k8nCm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTjl5mJT8uUw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "1NHq8hNe9EJg",
    "outputId": "2fb0845f-261f-4d62-a262-1aa779c0a460"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('variables.csv')\n",
    "df = df.drop(columns=['Epic_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sr3EOAcd9GR-",
    "outputId": "32fe3899-239a-4764-807b-ae2d317a450f"
   },
   "outputs": [],
   "source": [
    "n_lag = 4\n",
    "n_steps = 8\n",
    "# Create lagged variables for the deseas count, temperature, and rainfall\n",
    "columns = list(df.columns)\n",
    "\n",
    "for col in columns:\n",
    "    for i in range(1, n_lag + 1):\n",
    "        df[f\"{col}_lag_{i}\"] = df[col].shift(i)\n",
    "\n",
    "# Drop rows with NaN values generated by shifting\n",
    "df = df.dropna()\n",
    "\n",
    "# Only keep the target column when there is no lag\n",
    "df = df.drop(df.columns[1:50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SndCsmuN9q12"
   },
   "outputs": [],
   "source": [
    "y = df['Infectious and Parasitic Diseases'].shift(-n_steps+1)\n",
    "y = y.dropna()\n",
    "if n_steps != 1:\n",
    "  df = df.iloc[:-n_steps+1, :]\n",
    "\n",
    "X = df.drop('Infectious and Parasitic Diseases', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBz6WGs0_ucQ"
   },
   "outputs": [],
   "source": [
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYw4ifWhBCJC"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7BnxTj3Cl5p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, input_size=200, hidden_layer_size=16, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.conv1 = nn.Conv1d(1, 4, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(4, 24, kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(72, 8)\n",
    "        self.fc2 = nn.Linear(8, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
    "\n",
    "        conv_out = self.conv1(lstm_out)\n",
    "        conv_out = self.relu1(conv_out)\n",
    "        conv_out = self.pool1(conv_out)\n",
    "\n",
    "        conv_out = self.conv2(conv_out)\n",
    "        conv_out = self.relu2(conv_out)\n",
    "        conv_out = self.pool2(conv_out)\n",
    "\n",
    "        fc_out = self.fc1(conv_out.view(len(input_seq), -1))\n",
    "        predictions = self.fc2(fc_out)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_size = int(len(X) * 0.7)\n",
    "test_ind = range(train_size, len(X))\n",
    "\n",
    "y_pred_store = np.array([None] * len(X))\n",
    "y_test_array = np.array([])\n",
    "y_pred_array = np.array([])\n",
    "\n",
    "for forecast_ind in test_ind:\n",
    "    train_ind = list(range(0, int(forecast_ind * 0.85)))\n",
    "    val_ind = list(range(int(forecast_ind * 0.85), forecast_ind))\n",
    "    X_train = X[train_ind]\n",
    "    X_val = X[val_ind]\n",
    "    X_test = X[forecast_ind].reshape(1, -1)\n",
    "    y_train = y[train_ind]\n",
    "    y_val = y[val_ind]\n",
    "    y_test = y[forecast_ind]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)  # Scale validation data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = LSTM_CNN().to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "\n",
    "    train_data = TimeSeriesDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "    val_data = TimeSeriesDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())  # Validation data\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=False, drop_last=False)\n",
    "    val_loader = DataLoader(val_data, batch_size=16, shuffle=False, drop_last=False)  # Validation loader\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Set initial best validation loss to infinity\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):  # set the number of epochs\n",
    "        model.train()\n",
    "        for seq, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(seq.to(device))\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                y_val_pred = model(seq.to(device))\n",
    "                labels = labels.to(device).unsqueeze(1)\n",
    "                val_loss = loss_function(y_val_pred, labels).item()\n",
    "                val_losses.append(val_loss)\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            best_val_loss = mean_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt') \n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pt'))  # Load the best model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seq = torch.from_numpy(X_test).float().to(device)\n",
    "        y_test_pred = model(seq)\n",
    "        y_pred_store[forecast_ind] = y_test_pred.item()\n",
    "        y_test_array = np.append(y_test_array, y_test)\n",
    "        y_pred_array = np.append(y_pred_array, y_test_pred.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WpWl1E376do",
    "outputId": "4aaecc6e-6099-4a31-b122-37d190d972f5"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean squared error on training and test sets\n",
    "\n",
    "mse = mean_squared_error(y_test_array, y_pred_array)\n",
    "print('Mean Squared Error for {} step: {}'.format(n_steps ,mse))\n",
    "\n",
    "mae = mean_absolute_error(y_test_array, y_pred_array)\n",
    "print('Mean Absolute Error for {} step: {}'.format(n_steps ,mae))\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test_array, y_pred_array)\n",
    "print('Mean Absolute Percentage Error for {} step: {:.2f}%'.format(n_steps,mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GusK1g-aoDsg",
    "outputId": "98383cfc-5b45-425b-bd7a-e1efd6d63535"
   },
   "outputs": [],
   "source": [
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "\n",
    "    # Calculate absolute error of the forecasted values\n",
    "    abs_err = np.abs(y_true[n_steps:] - y_pred[n_steps:])\n",
    "\n",
    "    # Calculate the mean absolute error of the forecasted values\n",
    "    mae = np.mean(abs_err)\n",
    "\n",
    "    # Calculate the mean absolute error for the in-sample h-step naive forecast\n",
    "    naive_forecast = y_true[:-n_steps]  # naive forecast shifts the series by h step\n",
    "    mae_naive = np.mean(np.abs(y_true[n_steps:] - naive_forecast))\n",
    "\n",
    "    # Calculate and return MASE\n",
    "    mase = mae / mae_naive\n",
    "\n",
    "    return mase\n",
    "\n",
    "mase = mean_absolute_scaled_error(y_test_array, y_pred_array)\n",
    "print('Mean Absolute Scaled Error for {} step: {:.2f}'.format(n_steps,mase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DhrW5bAtVcM"
   },
   "outputs": [],
   "source": [
    "# 8 forecasting window. different size of samples.\n",
    "# 1 week ahead: [0, 48, 100, 152, 204, 256, 309, 361, 413, 465, 517]\n",
    "# 2 week ahead: [0, 47, 99, 151, 203, 255, 308, 360, 412, 464, 516]\n",
    "# 3 week ahead: [0, 46, 98, 150, 202, 254, 307, 359, 411, 463, 515]\n",
    "# 4 week ahead: [0, 45, 97, 149, 201, 253, 306, 358, 410, 462, 514]\n",
    "# 5 week ahead: [0, 44, 96, 148, 200, 252, 305, 357, 409, 461, 513]\n",
    "# 6 week ahead: [0, 43, 95, 147, 199, 251, 304, 356, 408, 460, 512]\n",
    "# 7 week ahead: [0, 42, 94, 146, 198, 250, 303, 355, 407, 459, 511]\n",
    "# 8 week ahead: [0, 41, 93, 145, 197, 249, 302, 354, 406, 458, 510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cixu4H0opabt",
    "outputId": "71cef753-6e7d-47f1-bbdb-7824e21221ab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "# Set the x-axis limits\n",
    "plt.xlim(-30, len(X)+40)\n",
    "\n",
    "# Shade the left half of the figure in light blue\n",
    "first_test_index = test_ind[0]  # Get the index of the first test observation\n",
    "plt.axvspan(-30, first_test_index, facecolor='lightblue', alpha=0.4)\n",
    "\n",
    "# Create tick locations and labels\n",
    "tick_interval = 52\n",
    "# origin data is [0, 52, 104, 156, 208, 260, 313, 365, 417, 469, 521]\n",
    "tick_locations = np.array([0, 41, 93, 145, 197, 249, 302, 354, 406, 458, 510])\n",
    "tick_label_locations = tick_locations[:-1] + tick_interval / 2\n",
    "tick_labels = [f'{year:02d}' for year in range(9, 19)]\n",
    "\n",
    "plt.plot(y, label='Observed', linestyle='-', linewidth=0.6, alpha=1, color='black')\n",
    "plt.scatter(range(len(y_pred_store)), y_pred_store, label=F'LSTM-CNN {n_steps} week ahead forecast', marker='o', s=10, alpha=1, color='darkorchid')\n",
    "\n",
    "\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('IPDs ED Attendances', fontsize=12)\n",
    "\n",
    "plt.xticks(tick_locations)\n",
    "plt.gca().set_xticklabels([])\n",
    "plt.gca().set_xticks(tick_label_locations, minor=True)\n",
    "plt.gca().set_xticklabels(tick_labels, minor=True)\n",
    "plt.gca().tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "plt.savefig('LSTMCNN8.tif', format='tif', dpi=400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5ObTdkPaiHZ",
    "outputId": "2121f5fd-a924-4258-d5d9-39ee0dc9c2df"
   },
   "outputs": [],
   "source": [
    "# # （without CV） just had a try. It is not our main code\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, X_data, y_data):\n",
    "#         self.X_data = X_data\n",
    "#         self.y_data = y_data\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.X_data[index], self.y_data[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X_data)\n",
    "\n",
    "# class LSTM_CNN(nn.Module):\n",
    "#     def __init__(self, input_size=200, hidden_layer_size=16, output_size=1, num_layers=2):\n",
    "#         super().__init__()\n",
    "#         self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "#         self.conv1 = nn.Conv1d(1, 4, kernel_size=3)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool1d(2)\n",
    "#         self.conv2 = nn.Conv1d(4, 24, kernel_size=2)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool1d(2)\n",
    "#         self.fc1 = nn.Linear(72, 8)\n",
    "#         self.fc2 = nn.Linear(8, output_size)\n",
    "\n",
    "#     def forward(self, input_seq):\n",
    "#         lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
    "\n",
    "#         conv_out = self.conv1(lstm_out)\n",
    "#         conv_out = self.relu1(conv_out)\n",
    "#         conv_out = self.pool1(conv_out)\n",
    "\n",
    "#         conv_out = self.conv2(conv_out)\n",
    "#         conv_out = self.relu2(conv_out)\n",
    "#         conv_out = self.pool2(conv_out)\n",
    "\n",
    "#         fc_out = self.fc1(conv_out.view(len(input_seq), -1))\n",
    "#         predictions = self.fc2(fc_out)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_size = int(len(X) * 0.7)\n",
    "# test_ind = range(train_size, len(X))\n",
    "\n",
    "# y_test_array = np.array([])\n",
    "# y_pred_array = np.array([])\n",
    "\n",
    "# train_ind = list(range(0, int(train_size * 0.85)))\n",
    "# val_ind = list(range(int(train_size * 0.85), train_size))\n",
    "# test_ind = list(range(train_size, len(X)))\n",
    "# X_train = X[train_ind]\n",
    "# X_val = X[val_ind]\n",
    "# X_test = X[test_ind]\n",
    "# y_train = y[train_ind]\n",
    "# y_val = y[val_ind]\n",
    "# y_test = y[test_ind]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)  # Scale validation data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# model = LSTM_CNN().to(device)\n",
    "# loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-3)\n",
    "\n",
    "# train_data = TimeSeriesDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "# val_data = TimeSeriesDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())  # Validation data\n",
    "# test_data = TimeSeriesDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "# train_loader = DataLoader(train_data, batch_size=16, shuffle=False, drop_last=False)\n",
    "# val_loader = DataLoader(val_data, batch_size=16, shuffle=False, drop_last=False)  # Validation loader\n",
    "# test_loader = DataLoader(test_data, batch_size=16, shuffle=False, drop_last=False)\n",
    "\n",
    "# num_epochs = 200\n",
    "# best_val_loss = float(\"inf\")  # Set initial best validation loss to infinity\n",
    "# train_draw = []\n",
    "# val_draw = []\n",
    "# best_epoch = 0\n",
    "\n",
    "# for epoch in range(num_epochs):  # set the number of epochs\n",
    "#     model.train()\n",
    "#     mean_train_loss = 0.0\n",
    "#     train_losses = []\n",
    "#     for seq, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         y_pred = model(seq.to(device))\n",
    "#         labels = labels.to(device).unsqueeze(1)\n",
    "#         single_loss = loss_function(y_pred, labels)\n",
    "#         single_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_losses.append(single_loss)\n",
    "#     mean_train_loss = torch.stack(train_losses).mean()\n",
    "#     mean_train_loss = mean_train_loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "#     model.eval()\n",
    "#     val_losses = []\n",
    "#     with torch.no_grad():\n",
    "#         for seq, labels in val_loader:\n",
    "#             y_val_pred = model(seq.to(device))\n",
    "#             labels = labels.to(device).unsqueeze(1)\n",
    "#             val_loss = loss_function(y_val_pred, labels).item()\n",
    "#             val_losses.append(val_loss)\n",
    "#         mean_val_loss = np.mean(val_losses)\n",
    "\n",
    "#     train_draw.append(mean_train_loss)\n",
    "#     val_draw.append(mean_val_loss)\n",
    "\n",
    "#     print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "#           .format(epoch+1, num_epochs, mean_train_loss, mean_val_loss))\n",
    "\n",
    "#     if mean_val_loss < best_val_loss:\n",
    "#         best_val_loss = mean_val_loss\n",
    "#         best_epoch = epoch\n",
    "#         torch.save(model.state_dict(), 'best_model.pt')  # Save the model\n",
    "\n",
    "# print('The epoch corresponding to the optimal validation loss is {}'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nmEylW3gPje",
    "outputId": "fdb4dbb2-65de-4764-bb2e-338c55cde64d"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_model.pt'))\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# outputs_list = []\n",
    "# targets_list = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in test_loader:\n",
    "#         outputs = model(inputs.to(device))\n",
    "#         targets = targets.to(device).unsqueeze(1)\n",
    "#         loss = loss_function(outputs, targets)\n",
    "#         test_loss += loss.item() * inputs.size(0)\n",
    "#         outputs_list.extend(outputs.detach().cpu().numpy())\n",
    "#         targets_list.extend(targets.detach().cpu().numpy())\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "# outputs_arr = np.array(outputs_list)\n",
    "# targets_arr = np.array(targets_list)\n",
    "\n",
    "\n",
    "# print('Test Loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtPhGzz_h5EW",
    "outputId": "b8cc8c32-6aef-490a-a085-46d6dd075836"
   },
   "outputs": [],
   "source": [
    "# def mape(actual, predicted):\n",
    "#     mask = actual != 0\n",
    "#     return np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100\n",
    "\n",
    "# mape_value = mape(targets_arr, outputs_arr)\n",
    "# print(\"MAPE: {:.2f}%\".format(mape_value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
